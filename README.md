# testgithub


åŒå¡”æ¨¡å‹å®ç°å‰äº¤å‰ï¼šç”Ÿæˆqueryæˆ–è€…userå¡”çš„ä¼ªå‘é‡ï¼Œç„¶åå’Œitemå¡”è¿›è¡Œæå‰äº¤äº’ï¼Œå¾—åˆ°æœ€ç»ˆçš„itemå‘é‡ã€‚
<img width="329" alt="image" src="https://github.com/user-attachments/assets/5bb93e6d-9804-4c98-a406-50e39fee122a">

<img width="299" alt="image" src="https://github.com/user-attachments/assets/f6f6d8dd-d3a6-44d1-9674-94c41b1e6bca">



LTVé¢„ä¼°æˆ–è€…è§‚çœ‹æ—¶é•¿é¢„ä¼°ã€‚
æœ€ç»ˆæ¶ˆè´¹é‡‘é¢é¢„ä¼°ï¼Œé‚£ä¹ˆé¢„ä¼°LTVè¿™ä¸ªå›å½’é—®é¢˜çš„æ ¸å¿ƒéš¾ç‚¹åœ¨äºï¼šæ•´ä¸ªç¾¤ä½“çš„æ•°æ®åˆ†å¸ƒä¸­ï¼Œæ··åˆäº†å¦‚ï¼šæ³Šæ¾ã€æŒ‡æ•°ç­‰å¤šç§åˆ†å¸ƒã€‚å…¶å®åŒ…æ‹¬è§‚çœ‹æ—¶é•¿é¢„ä¼°ä¹Ÿå¸¸å¸¸æ˜¯ä¸è§„åˆ™çš„å›å½’åˆ†å¸ƒã€‚
è¿™å…¶ä¸­æœ‰ä¸€ä¸ªå¤§çš„é—®é¢˜æ˜¯0é™„è¿‘æ•°ç›®æ¯”è¾ƒå¤šï¼Œå…¶ä»–éƒ¨åˆ†å®¹æ˜“ä¸è§„åˆ™ï¼Œæœ€åè§‚çœ‹æ—¶é•¿ï¼Œæœ€ç»ˆæ¶ˆè´¹é‡‘é¢å¾ˆéš¾é¢„ä¼°ã€‚
ä¸‰ç§æ–¹æ³•ï¼š
1ï¼‰ä½¿ç”¨0è†¨èƒ€æ­£å¤ªåˆ†å¸ƒæŸå¤±å‡½æ•°ï¼Œå³åœ¨0é™„è¿‘ä½¿ç”¨äºŒç±»åˆ†å¸ƒï¼Œåœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨å›å½’åˆ†å¸ƒ
2ï¼‰æ•´ä½“æ ‡ç­¾å–å¯¹æ•°ï¼Œè¿™æ ·å¯ä»¥å˜æˆæ­£æ€åˆ†å¸ƒï¼Œå®¹æ˜“æ‹Ÿåˆã€‚
3ï¼‰æ—¶é•¿å›å½’å¾ˆéš¾é¢„ä¼°ï¼Œå¯ä»¥å˜æˆåˆ†ç±»æ¨¡å‹ï¼Œå³è¦æ±‚açš„è§‚çœ‹æ—¶é•¿æ¯”bçš„è§‚çœ‹æ—¶é•¿æ›´é•¿å°±è¡Œï¼Œ è¿™ä¸ªæ¯”è¾ƒå®¹æ˜“é¢„ä¼°ã€‚



memonetï¼š æ¢ç´¢æ¨èæ¨¡å‹çš„scaling lawï¼Œå…¶å®è¯æ˜äº†æ‰€æœ‰çš„äºŒé˜¶cross-featureæ›´æœ‰æ•ˆçš„ä½¿ç”¨æ–¹å¼ï¼Œæ˜¯å‹ç¼©ï¼ˆmulti-hashç¼–ç é¿å…å†²çªï¼‰å’Œé‡å»ºï¼ˆæ¯ä¸ªä½¿ç”¨mlpæˆ–è€…ä¸€é˜¶ç‰¹å¾attentionæ¥é‡å»ºï¼‰---ç¥ç»ç½‘ç»œç¬¬ä¸€æ€§åŸç†ã€‚
aï¼‰In order to explain the success of large language models, many recent works[2, 23, 28, 34] discuss the memorization and generalization ability of these models. Some works[10, 16, 24] show that big models in NLP have great capabilities to memorize factual world knowledge cross feature is almost the most important knowledge contained in data of CTR tasks.introducing an independent memory mechanism into CTR ranking model to learn and memorize cross featuresâ€™ representations
bï¼‰ç¼–ç å‹ç¼©ä½¿ç”¨"multi-hash codebook" ã€‚segments the representation of cross feature into ğ‘š chunks and each chunk indicated by a different hash function represents part of the complete information. This helps distinguish cross features from each other in the same codeword because the probability of sharing the same representation exponentially decreases with the increase of hash function number
cï¼‰é‡å»ºï¼Œä½¿ç”¨ç½‘ç»œé‡å»ºäºŒé˜¶äº¤å‰ç‰¹å¾çš„è¡¨ç¤ºï¼Œä¸€ç§æ˜¯mlpï¼Œä¸€ç§æ˜¯ä½¿ä¸€é˜¶ç‰¹å¾attentionæ¥é‡å»ºã€‚ç¬¬äºŒç§Attentive Memory Restoring(AMR)å°±æ˜¯åŸå§‹çš„ä¸€é˜¶ç‰¹å¾çš„embeddingåšattentionï¼Œæå–äºŒé˜¶cross-featureç‰¹å¾é‡Œè¾¹çš„ä¿¡æ¯ï¼Œä»¥å»é™¤å¤šä¸ªcross-featureåœ¨coding-booké‡Œè¾¹å†²çªçš„ä¿¡æ¯ã€‚We find AMR outperforms LMR when the codeword number is relatively small while LMR performs better if we continue to increase the codewordâ€™s number. T
dï¼‰. ç‰¹å¾ç¼©å‡ã€‚äºŒé˜¶ç‰¹å¾æœ‰å™ªå£°ï¼Œä¸”å¤ªå¤šã€‚ç”¨ä¸€é˜¶ç‰¹å¾åšattentionï¼Œå»é™¤ä¸€äº›äºŒé˜¶ç‰¹å¾ï¼ˆæŒ‰ç…§ä¸€é˜¶ç»´åº¦å»æ‰ï¼Œå³æŸäº›ç‰¹å¾ç›¸å…³çš„äºŒé˜¶ç‰¹å¾éƒ½å­˜åœ¨æˆ–è€…éƒ½ä¸å­˜åœ¨ï¼‰ã€‚
eï¼‰æ•ˆæœ auc+0.01
